{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size # list of 0s of blocksize\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch] # character index\n",
    "        X.append(context)\n",
    "        Y.append(ix) #\n",
    "        print(''.join(itos[i] for i in context), '--->',itos[ix])\n",
    "        context = context[1:] + [ix] # Take the first 2 items i context and add the ix at the end of the list\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0451,  0.2271],\n",
       "        [-0.4246,  0.5392],\n",
       "        [-0.5737, -0.8957],\n",
       "        [ 1.4986, -0.5988],\n",
       "        [-1.3220,  0.0404],\n",
       "        [-0.2435, -0.5964],\n",
       "        [ 0.6180, -0.9611],\n",
       "        [ 0.6871, -1.3060],\n",
       "        [ 1.3025, -0.3111],\n",
       "        [-0.4725,  0.9513],\n",
       "        [ 1.2088,  0.0408],\n",
       "        [-0.1670,  1.7796],\n",
       "        [ 0.1698,  1.0846],\n",
       "        [-2.4542,  1.6683],\n",
       "        [-0.1402, -0.4956],\n",
       "        [-0.3065,  0.7363],\n",
       "        [ 0.5563, -0.1821],\n",
       "        [-0.1230,  0.1824],\n",
       "        [-0.1040,  0.9632],\n",
       "        [ 0.5984,  0.0641],\n",
       "        [ 1.0266,  0.6260],\n",
       "        [ 0.6885,  0.8778],\n",
       "        [-0.7261,  1.4174],\n",
       "        [ 2.7825, -2.3553],\n",
       "        [ 0.0887,  0.1935],\n",
       "        [-1.1268,  0.3802],\n",
       "        [ 0.6110,  1.1021]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2435, -0.5964])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2435, -0.5964])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5),num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4986, -0.5988],\n",
       "        [-1.3220,  0.0404],\n",
       "        [-0.2435, -0.5964]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([3,4,5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4246,  0.5392])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4246,  0.5392])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer\n",
    "W1 = torch.randn((6,100)) # input will be emb.shape(x,y,z) ---> (y * z, The number of neurons we decide (hyperparameter))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m \u001b[38;5;241m+\u001b[39m b1 \u001b[38;5;66;03m# does not work cause emb.shape(32,3,2) while W1 is (6, 100)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m               \u001b[38;5;66;03m# 32, 3, 2 x\u001b[39;00m\n\u001b[1;32m      3\u001b[0m               \u001b[38;5;66;03m#  6, 100\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "emb @ W1 + b1 # does not work cause emb.shape(32,3,2) while W1 is (6, 100)\n",
    "              # 32, 3, 2 x\n",
    "              #  6, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = emb.view(-1,6) @ W1  + b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple ways to change the shape of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -0.2435, -0.5964],\n",
       "        [-1.0451,  0.2271, -0.2435, -0.5964, -2.4542,  1.6683],\n",
       "        [-0.2435, -0.5964, -2.4542,  1.6683, -2.4542,  1.6683],\n",
       "        [-2.4542,  1.6683, -2.4542,  1.6683, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -0.3065,  0.7363],\n",
       "        [-1.0451,  0.2271, -0.3065,  0.7363,  0.1698,  1.0846],\n",
       "        [-0.3065,  0.7363,  0.1698,  1.0846, -0.4725,  0.9513],\n",
       "        [ 0.1698,  1.0846, -0.4725,  0.9513, -0.7261,  1.4174],\n",
       "        [-0.4725,  0.9513, -0.7261,  1.4174, -0.4725,  0.9513],\n",
       "        [-0.7261,  1.4174, -0.4725,  0.9513, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -0.4246,  0.5392, -0.7261,  1.4174],\n",
       "        [-0.4246,  0.5392, -0.7261,  1.4174, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -0.4725,  0.9513],\n",
       "        [-1.0451,  0.2271, -0.4725,  0.9513,  0.5984,  0.0641],\n",
       "        [-0.4725,  0.9513,  0.5984,  0.0641, -0.4246,  0.5392],\n",
       "        [ 0.5984,  0.0641, -0.4246,  0.5392, -0.5737, -0.8957],\n",
       "        [-0.4246,  0.5392, -0.5737, -0.8957, -0.2435, -0.5964],\n",
       "        [-0.5737, -0.8957, -0.2435, -0.5964,  0.1698,  1.0846],\n",
       "        [-0.2435, -0.5964,  0.1698,  1.0846,  0.1698,  1.0846],\n",
       "        [ 0.1698,  1.0846,  0.1698,  1.0846, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271,  0.5984,  0.0641],\n",
       "        [-1.0451,  0.2271,  0.5984,  0.0641, -0.3065,  0.7363],\n",
       "        [ 0.5984,  0.0641, -0.3065,  0.7363,  0.5563, -0.1821],\n",
       "        [-0.3065,  0.7363,  0.5563, -0.1821,  1.3025, -0.3111],\n",
       "        [ 0.5563, -0.1821,  1.3025, -0.3111, -0.4725,  0.9513],\n",
       "        [ 1.3025, -0.3111, -0.4725,  0.9513, -0.4246,  0.5392]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]],1) # grab all examples, the [0,1,2] index, all indexes in 3rd dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_opt = emb.view(-1,6)\n",
    "cat_opt = torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(view_opt,cat_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good because if we want to change the block size we will haveto manually do it in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we should do torch.unbind https://pytorch.org/docs/stable/generated/torch.unbind.html\n",
    "which removes a tensor dimension\n",
    "\n",
    "Returns a tuple of all slices along a given dimension, already without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbind_opt = torch.unbind(emb,1) # Same as the list in cat options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.2435, -0.5964],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4725,  0.9513],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.5737, -0.8957],\n",
       "         [-0.2435, -0.5964],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.5563, -0.1821],\n",
       "         [ 1.3025, -0.3111]]),\n",
       " tensor([[-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.2435, -0.5964],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4725,  0.9513],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.5737, -0.8957],\n",
       "         [-0.2435, -0.5964],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.5563, -0.1821],\n",
       "         [ 1.3025, -0.3111],\n",
       "         [-0.4725,  0.9513]]),\n",
       " tensor([[-1.0451,  0.2271],\n",
       "         [-0.2435, -0.5964],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4725,  0.9513],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.5737, -0.8957],\n",
       "         [-0.2435, -0.5964],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.5563, -0.1821],\n",
       "         [ 1.3025, -0.3111],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.4246,  0.5392]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbind_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.2435, -0.5964],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4725,  0.9513],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.5737, -0.8957],\n",
       "         [-0.2435, -0.5964],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.5563, -0.1821],\n",
       "         [ 1.3025, -0.3111]]),\n",
       " tensor([[-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.2435, -0.5964],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4725,  0.9513],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.5737, -0.8957],\n",
       "         [-0.2435, -0.5964],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-1.0451,  0.2271],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.5563, -0.1821],\n",
       "         [ 1.3025, -0.3111],\n",
       "         [-0.4725,  0.9513]]),\n",
       " tensor([[-1.0451,  0.2271],\n",
       "         [-0.2435, -0.5964],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-2.4542,  1.6683],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.7261,  1.4174],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [-0.4725,  0.9513],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-0.5737, -0.8957],\n",
       "         [-0.2435, -0.5964],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [ 0.1698,  1.0846],\n",
       "         [-0.4246,  0.5392],\n",
       "         [-1.0451,  0.2271],\n",
       "         [ 0.5984,  0.0641],\n",
       "         [-0.3065,  0.7363],\n",
       "         [ 0.5563, -0.1821],\n",
       "         [ 1.3025, -0.3111],\n",
       "         [-0.4725,  0.9513],\n",
       "         [-0.4246,  0.5392]])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -0.2435, -0.5964],\n",
       "        [-1.0451,  0.2271, -0.2435, -0.5964, -2.4542,  1.6683],\n",
       "        [-0.2435, -0.5964, -2.4542,  1.6683, -2.4542,  1.6683],\n",
       "        [-2.4542,  1.6683, -2.4542,  1.6683, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -0.3065,  0.7363],\n",
       "        [-1.0451,  0.2271, -0.3065,  0.7363,  0.1698,  1.0846],\n",
       "        [-0.3065,  0.7363,  0.1698,  1.0846, -0.4725,  0.9513],\n",
       "        [ 0.1698,  1.0846, -0.4725,  0.9513, -0.7261,  1.4174],\n",
       "        [-0.4725,  0.9513, -0.7261,  1.4174, -0.4725,  0.9513],\n",
       "        [-0.7261,  1.4174, -0.4725,  0.9513, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -0.4246,  0.5392, -0.7261,  1.4174],\n",
       "        [-0.4246,  0.5392, -0.7261,  1.4174, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -0.4725,  0.9513],\n",
       "        [-1.0451,  0.2271, -0.4725,  0.9513,  0.5984,  0.0641],\n",
       "        [-0.4725,  0.9513,  0.5984,  0.0641, -0.4246,  0.5392],\n",
       "        [ 0.5984,  0.0641, -0.4246,  0.5392, -0.5737, -0.8957],\n",
       "        [-0.4246,  0.5392, -0.5737, -0.8957, -0.2435, -0.5964],\n",
       "        [-0.5737, -0.8957, -0.2435, -0.5964,  0.1698,  1.0846],\n",
       "        [-0.2435, -0.5964,  0.1698,  1.0846,  0.1698,  1.0846],\n",
       "        [ 0.1698,  1.0846,  0.1698,  1.0846, -0.4246,  0.5392],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271, -1.0451,  0.2271],\n",
       "        [-1.0451,  0.2271, -1.0451,  0.2271,  0.5984,  0.0641],\n",
       "        [-1.0451,  0.2271,  0.5984,  0.0641, -0.3065,  0.7363],\n",
       "        [ 0.5984,  0.0641, -0.3065,  0.7363,  0.5563, -0.1821],\n",
       "        [-0.3065,  0.7363,  0.5563, -0.1821,  1.3025, -0.3111],\n",
       "        [ 0.5563, -0.1821,  1.3025, -0.3111, -0.4725,  0.9513],\n",
       "        [ 1.3025, -0.3111, -0.4725,  0.9513, -0.4246,  0.5392]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(unbind_opt,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a.view(3,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_418811/214256462.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  a.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ras = a.view(3,3,2)\n",
    "ras.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(-1,6) @ W1+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32, 100\n",
    "#  1, 100\n",
    "# This will copy vertically for every one of the 32 rows the 1, 100 vector and do an element wise addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100,27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1,keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.2366e-05, 9.3566e-10, 5.0658e-05, 1.2492e-12, 3.8561e-04, 3.0587e-06,\n",
       "         9.8584e-01, 9.6187e-03, 1.2000e-04, 2.8846e-07, 3.3360e-06, 1.9968e-12,\n",
       "         3.4284e-08, 1.2834e-06, 4.4725e-04, 6.5964e-04, 1.1321e-07, 9.9710e-13,\n",
       "         3.4790e-11, 8.0023e-13, 1.3880e-06, 8.9050e-10, 2.7877e-03, 3.1887e-05,\n",
       "         4.5079e-09, 5.0675e-07, 2.9620e-08],\n",
       "        [3.2929e-04, 2.6412e-09, 2.0807e-04, 8.5551e-12, 1.0617e-04, 1.6208e-06,\n",
       "         1.4771e-02, 8.3661e-01, 2.9591e-08, 4.2464e-06, 1.9077e-06, 6.1773e-09,\n",
       "         1.9069e-08, 3.5321e-07, 2.9850e-02, 5.4410e-02, 1.1778e-08, 3.7361e-10,\n",
       "         2.9665e-07, 1.9469e-09, 6.9641e-06, 5.5433e-06, 6.2688e-02, 8.0127e-07,\n",
       "         3.5140e-09, 5.7847e-10, 1.0026e-03],\n",
       "        [7.1652e-10, 1.2589e-11, 4.7370e-10, 3.8557e-12, 2.9714e-05, 2.3954e-03,\n",
       "         2.3456e-02, 7.2527e-06, 9.6175e-01, 3.0604e-10, 1.0164e-06, 1.9441e-13,\n",
       "         1.7471e-05, 1.2303e-02, 1.3284e-07, 1.1836e-05, 6.4235e-06, 5.0250e-12,\n",
       "         5.1195e-20, 2.6918e-16, 2.3801e-08, 3.4769e-12, 1.4416e-05, 3.1312e-06,\n",
       "         7.5479e-09, 8.4330e-09, 1.6618e-13],\n",
       "        [8.6352e-06, 3.1637e-09, 1.2436e-05, 6.3958e-09, 9.0590e-06, 8.0721e-08,\n",
       "         8.3548e-01, 1.3045e-01, 1.8049e-04, 1.1307e-06, 9.9295e-05, 8.1807e-13,\n",
       "         7.6298e-03, 6.9729e-09, 2.1148e-02, 7.9814e-06, 1.8695e-05, 1.1699e-10,\n",
       "         7.9833e-08, 1.7238e-12, 4.3955e-05, 1.5724e-06, 8.8185e-05, 2.4734e-04,\n",
       "         1.1895e-03, 3.3861e-03, 1.3297e-06],\n",
       "        [7.4720e-09, 2.9604e-12, 3.1913e-08, 4.9039e-10, 2.0778e-07, 1.7366e-12,\n",
       "         8.2173e-04, 9.9042e-01, 4.3966e-15, 7.4582e-05, 2.2189e-07, 3.7800e-12,\n",
       "         2.1193e-09, 2.7368e-10, 2.6907e-04, 1.0220e-03, 3.4713e-11, 7.6614e-14,\n",
       "         2.3799e-08, 4.9954e-09, 6.3578e-05, 3.1562e-09, 7.3277e-03, 3.9568e-07,\n",
       "         1.3627e-06, 4.8765e-07, 1.1355e-07],\n",
       "        [5.2366e-05, 9.3566e-10, 5.0658e-05, 1.2492e-12, 3.8561e-04, 3.0587e-06,\n",
       "         9.8584e-01, 9.6187e-03, 1.2000e-04, 2.8846e-07, 3.3360e-06, 1.9968e-12,\n",
       "         3.4284e-08, 1.2834e-06, 4.4725e-04, 6.5964e-04, 1.1321e-07, 9.9710e-13,\n",
       "         3.4790e-11, 8.0023e-13, 1.3880e-06, 8.9050e-10, 2.7877e-03, 3.1887e-05,\n",
       "         4.5079e-09, 5.0675e-07, 2.9620e-08],\n",
       "        [2.7881e-07, 5.5267e-10, 2.2950e-05, 1.3976e-11, 7.6647e-07, 1.0132e-09,\n",
       "         9.8197e-01, 8.9728e-06, 5.7325e-09, 1.0365e-07, 2.0776e-08, 4.9419e-14,\n",
       "         5.1953e-11, 2.2664e-07, 7.2978e-06, 4.0707e-03, 1.8091e-05, 3.0876e-12,\n",
       "         4.3544e-10, 4.0718e-14, 1.1296e-02, 2.9543e-08, 2.1822e-03, 3.5740e-07,\n",
       "         2.4642e-05, 3.9342e-04, 2.6676e-07],\n",
       "        [1.0647e-12, 4.2758e-13, 4.2379e-09, 5.6022e-11, 6.8577e-08, 2.0773e-12,\n",
       "         2.3265e-03, 2.7235e-09, 1.1204e-14, 6.9574e-07, 4.7500e-10, 1.7291e-15,\n",
       "         1.1009e-13, 2.7732e-08, 2.8279e-09, 2.4490e-04, 6.2960e-05, 1.3218e-13,\n",
       "         5.6218e-15, 3.8550e-13, 9.9722e-01, 7.4632e-10, 8.5954e-05, 1.4176e-10,\n",
       "         5.7517e-05, 1.3039e-07, 8.3461e-13],\n",
       "        [1.9740e-11, 1.6853e-13, 5.6337e-13, 2.9837e-08, 1.7136e-06, 3.2730e-09,\n",
       "         3.1874e-01, 5.6402e-07, 1.1339e-11, 6.9559e-07, 6.5191e-09, 3.7955e-15,\n",
       "         2.9953e-10, 2.4203e-06, 2.5002e-07, 1.7548e-06, 5.3087e-06, 5.0496e-11,\n",
       "         5.2988e-18, 6.7398e-09, 1.1558e-02, 3.2039e-09, 6.5847e-01, 3.7019e-07,\n",
       "         1.1222e-02, 8.4369e-09, 2.8679e-12],\n",
       "        [5.5341e-12, 2.4596e-17, 3.5928e-16, 1.2198e-12, 3.0886e-08, 1.4721e-09,\n",
       "         9.9825e-01, 1.5914e-10, 8.5954e-12, 3.0150e-08, 6.2435e-13, 2.8516e-17,\n",
       "         6.1606e-09, 2.9067e-11, 8.7074e-12, 2.8132e-09, 5.7130e-09, 6.6207e-11,\n",
       "         7.5728e-19, 2.3477e-11, 2.3004e-07, 2.7692e-11, 1.4634e-03, 2.0653e-07,\n",
       "         2.8918e-04, 4.6969e-09, 9.0785e-13],\n",
       "        [1.5470e-09, 7.0054e-14, 5.3623e-12, 1.2151e-09, 9.8587e-08, 1.4600e-10,\n",
       "         5.6068e-02, 3.0549e-03, 4.2316e-13, 2.3623e-06, 4.0890e-10, 2.0350e-14,\n",
       "         2.1729e-09, 1.4382e-10, 1.2517e-06, 2.2772e-05, 1.3871e-07, 2.2341e-10,\n",
       "         4.4672e-13, 6.1340e-09, 1.9020e-02, 9.4527e-09, 9.1839e-01, 6.0918e-08,\n",
       "         3.4415e-03, 2.0888e-09, 2.0169e-10],\n",
       "        [2.6452e-10, 1.1127e-15, 1.0968e-13, 8.7678e-12, 5.6749e-07, 1.5480e-10,\n",
       "         2.9648e-02, 6.9822e-05, 1.7439e-13, 2.3066e-08, 1.1397e-10, 4.1882e-15,\n",
       "         4.4650e-11, 2.4084e-10, 1.3616e-06, 2.8622e-06, 5.6036e-11, 2.4364e-11,\n",
       "         1.2129e-13, 2.4011e-09, 2.6691e-05, 2.9783e-12, 9.6947e-01, 3.2187e-09,\n",
       "         7.7587e-04, 1.8660e-11, 1.8374e-11],\n",
       "        [5.2366e-05, 9.3566e-10, 5.0658e-05, 1.2492e-12, 3.8561e-04, 3.0587e-06,\n",
       "         9.8584e-01, 9.6187e-03, 1.2000e-04, 2.8846e-07, 3.3360e-06, 1.9968e-12,\n",
       "         3.4284e-08, 1.2834e-06, 4.4725e-04, 6.5964e-04, 1.1321e-07, 9.9710e-13,\n",
       "         3.4790e-11, 8.0023e-13, 1.3880e-06, 8.9050e-10, 2.7877e-03, 3.1887e-05,\n",
       "         4.5079e-09, 5.0675e-07, 2.9620e-08],\n",
       "        [1.2991e-06, 4.7080e-10, 3.8675e-05, 1.0679e-11, 3.9372e-06, 4.0275e-09,\n",
       "         9.8891e-01, 3.7176e-05, 1.2478e-08, 1.6087e-07, 6.5810e-08, 1.4839e-13,\n",
       "         2.7502e-10, 2.3830e-07, 3.2603e-05, 5.3332e-03, 6.8257e-06, 3.4449e-12,\n",
       "         9.6299e-10, 4.4879e-13, 1.6290e-03, 1.1500e-08, 3.8761e-03, 1.4232e-06,\n",
       "         4.4094e-06, 1.2040e-04, 6.9310e-07],\n",
       "        [1.0149e-11, 5.2786e-10, 2.2639e-09, 8.8201e-11, 2.6713e-05, 1.7645e-08,\n",
       "         5.2602e-01, 5.0090e-08, 1.6491e-09, 1.0323e-07, 1.1527e-07, 1.8124e-15,\n",
       "         9.6939e-09, 1.0568e-04, 4.0298e-09, 2.9589e-04, 8.2810e-05, 9.3700e-11,\n",
       "         3.8846e-16, 3.3255e-15, 4.0558e-01, 3.4405e-10, 6.7882e-02, 7.2562e-08,\n",
       "         7.7761e-06, 2.7262e-06, 9.5351e-11],\n",
       "        [3.1740e-08, 3.4233e-12, 1.2304e-10, 8.5984e-09, 1.4708e-08, 1.3389e-10,\n",
       "         5.1083e-02, 2.0309e-01, 9.6411e-13, 5.6337e-05, 2.6700e-09, 4.7121e-12,\n",
       "         4.1588e-10, 1.3812e-09, 6.2585e-04, 7.5460e-04, 5.5058e-07, 3.1212e-07,\n",
       "         2.2810e-12, 1.1390e-08, 2.2449e-01, 6.0246e-06, 5.1304e-01, 1.7508e-07,\n",
       "         6.8534e-03, 1.0934e-09, 2.4869e-09],\n",
       "        [5.2366e-05, 9.3566e-10, 5.0658e-05, 1.2492e-12, 3.8561e-04, 3.0587e-06,\n",
       "         9.8584e-01, 9.6187e-03, 1.2000e-04, 2.8846e-07, 3.3360e-06, 1.9968e-12,\n",
       "         3.4284e-08, 1.2834e-06, 4.4725e-04, 6.5964e-04, 1.1321e-07, 9.9710e-13,\n",
       "         3.4790e-11, 8.0023e-13, 1.3880e-06, 8.9050e-10, 2.7877e-03, 3.1887e-05,\n",
       "         4.5079e-09, 5.0675e-07, 2.9620e-08],\n",
       "        [4.7020e-08, 4.9941e-10, 8.4131e-06, 6.4957e-12, 1.2874e-06, 1.2598e-09,\n",
       "         9.9411e-01, 1.0571e-06, 9.6675e-09, 3.4733e-08, 9.1249e-09, 1.0927e-14,\n",
       "         6.9328e-11, 3.4480e-07, 1.2775e-06, 9.7139e-04, 7.9304e-06, 2.8371e-12,\n",
       "         3.2641e-11, 2.8531e-15, 4.3532e-03, 9.1097e-09, 2.7354e-04, 1.7687e-07,\n",
       "         8.9965e-06, 2.6507e-04, 1.2335e-07],\n",
       "        [1.4372e-10, 2.5046e-09, 3.8052e-09, 1.0924e-07, 1.3742e-08, 1.3394e-12,\n",
       "         4.1257e-04, 7.0963e-06, 2.1470e-16, 3.9672e-06, 7.2725e-10, 9.5406e-12,\n",
       "         7.1071e-14, 5.6785e-07, 3.3780e-07, 1.2546e-03, 1.4926e-05, 3.4589e-10,\n",
       "         6.1748e-10, 1.8395e-10, 9.9687e-01, 6.4687e-05, 1.2598e-03, 6.9934e-10,\n",
       "         1.1078e-04, 1.1463e-09, 3.9157e-09],\n",
       "        [2.8075e-12, 5.2530e-13, 1.6042e-17, 1.0438e-09, 3.1885e-08, 9.6274e-08,\n",
       "         2.8407e-07, 6.4674e-12, 7.3137e-12, 6.5667e-12, 5.0410e-11, 9.0346e-16,\n",
       "         1.7860e-13, 2.6553e-07, 5.7595e-12, 3.1528e-12, 5.8661e-14, 2.0228e-14,\n",
       "         7.8859e-22, 2.0176e-18, 3.7866e-12, 1.6039e-15, 1.0000e+00, 9.5745e-14,\n",
       "         1.3516e-09, 2.9156e-11, 1.5383e-14],\n",
       "        [3.7087e-05, 8.0790e-13, 1.1165e-09, 7.0576e-10, 1.5755e-04, 2.6021e-06,\n",
       "         3.8525e-04, 1.8308e-03, 3.7841e-04, 3.0578e-06, 5.4266e-06, 2.1904e-07,\n",
       "         2.2458e-10, 1.6868e-07, 2.5122e-01, 1.2163e-05, 3.9774e-08, 3.6345e-06,\n",
       "         5.0776e-10, 5.9724e-06, 5.2020e-08, 1.0749e-02, 7.3519e-01, 1.1670e-08,\n",
       "         1.9606e-05, 3.5270e-13, 1.0672e-06],\n",
       "        [5.2718e-05, 3.4138e-10, 1.2977e-05, 1.4969e-13, 2.6648e-04, 2.2503e-02,\n",
       "         1.3725e-05, 3.9863e-08, 7.5568e-04, 6.6176e-11, 2.0444e-06, 1.2834e-08,\n",
       "         5.8854e-12, 2.4658e-09, 1.7639e-05, 8.2758e-07, 2.3783e-08, 7.0571e-10,\n",
       "         1.0125e-10, 1.9807e-11, 5.7411e-12, 1.1633e-08, 9.6463e-01, 2.5039e-10,\n",
       "         1.3340e-09, 4.9857e-09, 1.1744e-02],\n",
       "        [1.6125e-10, 5.3225e-08, 3.6233e-04, 5.2585e-09, 4.9385e-10, 5.0738e-08,\n",
       "         1.0331e-02, 1.1626e-11, 3.0355e-07, 1.0992e-05, 1.2279e-11, 2.0866e-14,\n",
       "         1.1849e-09, 2.9603e-06, 3.9384e-06, 2.5193e-02, 2.4244e-02, 4.3302e-12,\n",
       "         9.9883e-13, 1.1518e-14, 7.9421e-04, 1.7450e-12, 2.1492e-11, 1.5862e-08,\n",
       "         6.6396e-07, 9.3906e-01, 4.6050e-07],\n",
       "        [6.4572e-12, 1.8439e-10, 2.8362e-08, 2.2694e-06, 3.5764e-10, 2.1245e-13,\n",
       "         8.8735e-02, 8.3176e-11, 3.9692e-14, 1.7246e-03, 1.0371e-12, 2.7371e-15,\n",
       "         1.2247e-13, 7.7137e-05, 1.1872e-08, 4.7207e-04, 3.7546e-04, 2.1238e-06,\n",
       "         1.1107e-14, 9.1168e-12, 9.0713e-01, 2.4874e-06, 1.9924e-09, 4.8319e-07,\n",
       "         1.1332e-03, 3.4836e-04, 7.8576e-13],\n",
       "        [3.4571e-11, 1.4970e-14, 2.6705e-16, 4.7368e-09, 5.4701e-07, 2.4123e-09,\n",
       "         7.1761e-01, 4.9159e-08, 5.6278e-10, 3.3815e-07, 8.7098e-11, 5.3144e-14,\n",
       "         3.5791e-10, 2.6375e-07, 3.3287e-07, 1.2541e-06, 2.0240e-07, 3.4754e-09,\n",
       "         3.7659e-16, 6.3389e-07, 8.0408e-05, 6.4977e-09, 1.6614e-01, 1.3171e-08,\n",
       "         1.1617e-01, 7.6388e-11, 5.7965e-11],\n",
       "        [5.2366e-05, 9.3566e-10, 5.0658e-05, 1.2492e-12, 3.8561e-04, 3.0587e-06,\n",
       "         9.8584e-01, 9.6187e-03, 1.2000e-04, 2.8846e-07, 3.3360e-06, 1.9968e-12,\n",
       "         3.4284e-08, 1.2834e-06, 4.4725e-04, 6.5964e-04, 1.1321e-07, 9.9710e-13,\n",
       "         3.4790e-11, 8.0023e-13, 1.3880e-06, 8.9050e-10, 2.7877e-03, 3.1887e-05,\n",
       "         4.5079e-09, 5.0675e-07, 2.9620e-08],\n",
       "        [1.1119e-06, 1.2197e-09, 6.6251e-05, 1.5881e-09, 5.2343e-08, 1.2658e-09,\n",
       "         6.9973e-02, 5.2354e-02, 7.1121e-12, 3.4855e-05, 4.5660e-07, 8.6477e-10,\n",
       "         6.8817e-12, 1.8268e-06, 3.1778e-03, 1.0080e-02, 4.5348e-04, 1.2257e-09,\n",
       "         7.0531e-05, 1.0345e-10, 6.9406e-01, 3.2338e-05, 1.6629e-01, 5.5777e-08,\n",
       "         3.3613e-03, 4.3044e-05, 5.4454e-06],\n",
       "        [5.8478e-09, 1.5242e-07, 6.4747e-11, 1.1623e-03, 4.4785e-05, 1.4593e-03,\n",
       "         5.6030e-03, 6.7844e-09, 3.6523e-09, 7.2207e-05, 4.3832e-07, 9.0650e-14,\n",
       "         1.3366e-07, 1.2607e-02, 3.3949e-08, 2.3856e-06, 6.0898e-10, 2.9155e-10,\n",
       "         5.6575e-17, 3.1850e-14, 2.4320e-06, 8.7985e-11, 9.7905e-01, 2.6108e-09,\n",
       "         1.0257e-07, 6.9893e-07, 1.1819e-10],\n",
       "        [7.7736e-09, 6.4366e-10, 3.5790e-09, 3.1693e-09, 1.8997e-10, 1.8570e-13,\n",
       "         2.9485e-03, 3.2855e-10, 9.5452e-12, 1.0450e-03, 9.6496e-12, 1.0905e-07,\n",
       "         3.0842e-12, 6.6590e-09, 2.1522e-05, 1.3520e-03, 2.3836e-02, 3.9628e-05,\n",
       "         1.3831e-07, 5.5139e-06, 1.3422e-05, 1.2211e-04, 5.6033e-06, 3.8765e-10,\n",
       "         9.7061e-01, 2.0844e-07, 5.9002e-09],\n",
       "        [2.3887e-05, 1.1018e-03, 3.6895e-09, 4.2110e-03, 6.2255e-08, 8.6903e-05,\n",
       "         1.6247e-07, 2.2140e-10, 1.2859e-09, 1.3788e-04, 1.3388e-08, 5.1251e-06,\n",
       "         1.1220e-10, 3.8897e-03, 5.9109e-09, 5.5255e-04, 2.6329e-06, 3.5289e-04,\n",
       "         6.1482e-08, 3.7711e-07, 8.2827e-05, 1.7801e-03, 1.6980e-01, 4.8156e-08,\n",
       "         1.3189e-06, 5.3272e-08, 8.1797e-01],\n",
       "        [2.3234e-07, 5.0442e-05, 1.4999e-10, 2.4850e-05, 6.8271e-06, 1.3671e-01,\n",
       "         1.3365e-07, 1.1814e-09, 1.8345e-02, 1.2139e-07, 5.7818e-06, 1.3746e-09,\n",
       "         1.4961e-07, 8.3450e-01, 7.8527e-10, 8.2421e-05, 4.1733e-05, 8.7916e-08,\n",
       "         1.6454e-14, 1.4985e-11, 6.8117e-10, 3.8974e-08, 1.0221e-02, 2.2206e-09,\n",
       "         6.4656e-06, 6.0014e-06, 4.0452e-10],\n",
       "        [1.8553e-08, 4.1036e-13, 8.1205e-10, 1.6642e-10, 7.8993e-11, 6.2678e-10,\n",
       "         5.0530e-02, 7.4914e-10, 3.6564e-07, 2.9383e-06, 3.2601e-11, 2.2575e-09,\n",
       "         4.4391e-08, 1.0411e-06, 6.5276e-04, 4.1093e-08, 9.5252e-04, 1.3587e-02,\n",
       "         2.0229e-09, 4.0326e-08, 2.0270e-06, 2.4931e-03, 4.3833e-08, 7.3143e-08,\n",
       "         9.3176e-01, 1.5711e-05, 7.3052e-12]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.0905)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32),Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More respectable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,2), generator = g)\n",
    "W1 = torch.randn((6,100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator = g)\n",
    "b2 = torch.randn(27,generator=g)\n",
    "params = [C,W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.255642831325531\n",
      "0.2556484341621399\n",
      "0.25563403964042664\n",
      "0.25563961267471313\n",
      "0.25562533736228943\n",
      "0.25563082098960876\n",
      "0.25561660528182983\n",
      "0.25562208890914917\n",
      "0.2556079626083374\n",
      "0.25561341643333435\n",
      "0.25559940934181213\n",
      "0.2556048333644867\n",
      "0.2555908262729645\n",
      "0.25559622049331665\n",
      "0.255582332611084\n",
      "0.255587637424469\n",
      "0.2555737793445587\n",
      "0.2555791437625885\n",
      "0.255565345287323\n",
      "0.255570650100708\n",
      "0.25555694103240967\n",
      "0.2555622458457947\n",
      "0.25554853677749634\n",
      "0.2555537521839142\n",
      "0.255540132522583\n",
      "0.2555452883243561\n",
      "0.25553178787231445\n",
      "0.25553691387176514\n",
      "0.2555234730243683\n",
      "0.2555285692214966\n",
      "0.2555152177810669\n",
      "0.2555202841758728\n",
      "0.2555069923400879\n",
      "0.2555120289325714\n",
      "0.2554987967014313\n",
      "0.2555038034915924\n",
      "0.25549057126045227\n",
      "0.2554956078529358\n",
      "0.2554824650287628\n",
      "0.2554874122142792\n",
      "0.25547435879707336\n",
      "0.2554793059825897\n",
      "0.2554663121700287\n",
      "0.25547119975090027\n",
      "0.2554582953453064\n",
      "0.2554631531238556\n",
      "0.2554502487182617\n",
      "0.2554550766944885\n",
      "0.2554422616958618\n",
      "0.25544705986976624\n",
      "0.2554343342781067\n",
      "0.2554391622543335\n",
      "0.2554264962673187\n",
      "0.25543123483657837\n",
      "0.255418598651886\n",
      "0.25542333722114563\n",
      "0.25541073083877563\n",
      "0.2554154396057129\n",
      "0.25540292263031006\n",
      "0.2554076015949249\n",
      "0.25539517402648926\n",
      "0.2553998529911041\n",
      "0.25538742542266846\n",
      "0.25539204478263855\n",
      "0.25537970662117004\n",
      "0.25538426637649536\n",
      "0.255372017621994\n",
      "0.2553766369819641\n",
      "0.2553643584251404\n",
      "0.2553689181804657\n",
      "0.25535669922828674\n",
      "0.2553612291812897\n",
      "0.2553490996360779\n",
      "0.2553535997867584\n",
      "0.2553415298461914\n",
      "0.25534600019454956\n",
      "0.2553339898586273\n",
      "0.2553384602069855\n",
      "0.25532644987106323\n",
      "0.255330890417099\n",
      "0.25531893968582153\n",
      "0.2553233504295349\n",
      "0.2553114891052246\n",
      "0.2553158700466156\n",
      "0.2553040683269501\n",
      "0.2553083896636963\n",
      "0.25529661774635315\n",
      "0.25530093908309937\n",
      "0.2552892565727234\n",
      "0.25529351830482483\n",
      "0.25528186559677124\n",
      "0.25528618693351746\n",
      "0.25527459383010864\n",
      "0.2552788257598877\n",
      "0.25526729226112366\n",
      "0.2552715241909027\n",
      "0.25525999069213867\n",
      "0.2552642226219177\n",
      "0.2552528381347656\n",
      "0.2552570402622223\n",
      "0.2552456855773926\n",
      "0.25524982810020447\n",
      "0.25523847341537476\n",
      "0.25524264574050903\n",
      "0.2552313506603241\n",
      "0.2552354335784912\n",
      "0.25522422790527344\n",
      "0.25522834062576294\n",
      "0.25521716475486755\n",
      "0.2552211880683899\n",
      "0.25521010160446167\n",
      "0.2552141547203064\n",
      "0.25520309805870056\n",
      "0.2552070915699005\n",
      "0.25519609451293945\n",
      "0.255200058221817\n",
      "0.25518912076950073\n",
      "0.2551931142807007\n",
      "0.255182147026062\n",
      "0.2551860809326172\n",
      "0.2551751434803009\n",
      "0.25517910718917847\n",
      "0.25516825914382935\n",
      "0.2551721930503845\n",
      "0.25516143441200256\n",
      "0.2551652789115906\n",
      "0.2551545202732086\n",
      "0.255158394575119\n",
      "0.2551477253437042\n",
      "0.25515156984329224\n",
      "0.25514090061187744\n",
      "0.25514474511146545\n",
      "0.25513413548469543\n",
      "0.25513795018196106\n",
      "0.2551273703575134\n",
      "0.25513115525245667\n",
      "0.2551206946372986\n",
      "0.25512444972991943\n",
      "0.25511395931243896\n",
      "0.25511765480041504\n",
      "0.2551072835922241\n",
      "0.2551110088825226\n",
      "0.2551006078720093\n",
      "0.25510433316230774\n",
      "0.2550940215587616\n",
      "0.25509771704673767\n",
      "0.25508740544319153\n",
      "0.2550911009311676\n",
      "0.25508084893226624\n",
      "0.25508448481559753\n",
      "0.25507426261901855\n",
      "0.25507786870002747\n",
      "0.2550676465034485\n",
      "0.2550712823867798\n",
      "0.25506117939949036\n",
      "0.2550647258758545\n",
      "0.2550547122955322\n",
      "0.25505825877189636\n",
      "0.2550482153892517\n",
      "0.25505173206329346\n",
      "0.2550417482852936\n",
      "0.2550453245639801\n",
      "0.2550353407859802\n",
      "0.25503888726234436\n",
      "0.25502896308898926\n",
      "0.2550325095653534\n",
      "0.25502270460128784\n",
      "0.25502610206604004\n",
      "0.2550162672996521\n",
      "0.2550197243690491\n",
      "0.2550099492073059\n",
      "0.2550134062767029\n",
      "0.2550036311149597\n",
      "0.2550070881843567\n",
      "0.2549973726272583\n",
      "0.2550007998943329\n",
      "0.2549910843372345\n",
      "0.2549944818019867\n",
      "0.25498488545417786\n",
      "0.2549882233142853\n",
      "0.25497862696647644\n",
      "0.25498202443122864\n",
      "0.2549723982810974\n",
      "0.25497573614120483\n",
      "0.25496622920036316\n",
      "0.2549695670604706\n",
      "0.25496014952659607\n",
      "0.2549634277820587\n",
      "0.2549540102481842\n",
      "0.25495728850364685\n",
      "0.2549479007720947\n",
      "0.2549511194229126\n",
      "0.25494176149368286\n",
      "0.2549450695514679\n",
      "0.25493568181991577\n",
      "0.25493893027305603\n",
      "0.25492966175079346\n",
      "0.2549329102039337\n",
      "0.25492364168167114\n",
      "0.2549268305301666\n",
      "0.25491762161254883\n",
      "0.2549208104610443\n",
      "0.2549116611480713\n",
      "0.2549148201942444\n",
      "0.25490570068359375\n",
      "0.25490880012512207\n",
      "0.2548997402191162\n",
      "0.25490283966064453\n",
      "0.25489377975463867\n",
      "0.25489693880081177\n",
      "0.2548878788948059\n",
      "0.25489097833633423\n",
      "0.2548820376396179\n",
      "0.25488516688346863\n",
      "0.25487619638442993\n",
      "0.25487929582595825\n",
      "0.25487035512924194\n",
      "0.25487345457077026\n",
      "0.25486457347869873\n",
      "0.25486764311790466\n",
      "0.25485876202583313\n",
      "0.25486183166503906\n",
      "0.25485295057296753\n",
      "0.25485602021217346\n",
      "0.2548472285270691\n",
      "0.25485026836395264\n",
      "0.2548414468765259\n",
      "0.25484442710876465\n",
      "0.25483569502830505\n",
      "0.2548386752605438\n",
      "0.254830002784729\n",
      "0.2548329830169678\n",
      "0.25482431054115295\n",
      "0.25482726097106934\n",
      "0.2548186480998993\n",
      "0.2548215687274933\n",
      "0.254813015460968\n",
      "0.2548159062862396\n",
      "0.25480738282203674\n",
      "0.25481027364730835\n",
      "0.25480177998542786\n",
      "0.25480467081069946\n",
      "0.25479623675346375\n",
      "0.2547990679740906\n",
      "0.25479063391685486\n",
      "0.2547934353351593\n",
      "0.25478503108024597\n",
      "0.2547878921031952\n",
      "0.25477954745292664\n",
      "0.25478240847587585\n",
      "0.2547740638256073\n",
      "0.25477686524391174\n",
      "0.2547685503959656\n",
      "0.2547713816165924\n",
      "0.25476303696632385\n",
      "0.25476589798927307\n",
      "0.2547575831413269\n",
      "0.25476041436195374\n",
      "0.25475215911865234\n",
      "0.254754900932312\n",
      "0.2547467052936554\n",
      "0.25474944710731506\n",
      "0.2547413110733032\n",
      "0.2547439932823181\n",
      "0.25473588705062866\n",
      "0.25473856925964355\n",
      "0.2547304630279541\n",
      "0.2547331750392914\n",
      "0.2547251582145691\n",
      "0.254727840423584\n",
      "0.2547197937965393\n",
      "0.2547224760055542\n",
      "0.2547144591808319\n",
      "0.2547171115875244\n",
      "0.2547091245651245\n",
      "0.2547118067741394\n",
      "0.25470390915870667\n",
      "0.25470659136772156\n",
      "0.25469866394996643\n",
      "0.25470128655433655\n",
      "0.2546934485435486\n",
      "0.2546960413455963\n",
      "0.25468817353248596\n",
      "0.2546907961368561\n",
      "0.2546829581260681\n",
      "0.25468555092811584\n",
      "0.2546777129173279\n",
      "0.254680335521698\n",
      "0.2546725571155548\n",
      "0.25467514991760254\n",
      "0.2546674311161041\n",
      "0.2546699643135071\n",
      "0.25466227531433105\n",
      "0.2546648383140564\n",
      "0.25465714931488037\n",
      "0.2546597123146057\n",
      "0.2546520233154297\n",
      "0.25465452671051025\n",
      "0.2546469271183014\n",
      "0.25464943051338196\n",
      "0.2546417713165283\n",
      "0.2546442747116089\n",
      "0.2546367347240448\n",
      "0.2546391785144806\n",
      "0.2546316385269165\n",
      "0.2546341121196747\n",
      "0.2546265721321106\n",
      "0.2546290457248688\n",
      "0.25462159514427185\n",
      "0.25462400913238525\n",
      "0.25461655855178833\n",
      "0.25461897253990173\n",
      "0.2546115517616272\n",
      "0.2546139359474182\n",
      "0.2546065151691437\n",
      "0.2546089291572571\n",
      "0.2546015679836273\n",
      "0.2546040117740631\n",
      "0.25459662079811096\n",
      "0.25459903478622437\n",
      "0.254591703414917\n",
      "0.254594087600708\n",
      "0.2545868158340454\n",
      "0.25458914041519165\n",
      "0.2545819580554962\n",
      "0.25458434224128723\n",
      "0.254577100276947\n",
      "0.25457945466041565\n",
      "0.25457218289375305\n",
      "0.2545745372772217\n",
      "0.25456735491752625\n",
      "0.2545697093009949\n",
      "0.25456249713897705\n",
      "0.2545648217201233\n",
      "0.25455769896507263\n",
      "0.2545599937438965\n",
      "0.2545529007911682\n",
      "0.25455519556999207\n",
      "0.2545481026172638\n",
      "0.25455042719841003\n",
      "0.25454333424568176\n",
      "0.2545456290245056\n",
      "0.2545386254787445\n",
      "0.2545408606529236\n",
      "0.2545338273048401\n",
      "0.25453609228134155\n",
      "0.25452905893325806\n",
      "0.25453129410743713\n",
      "0.2545243501663208\n",
      "0.2545265257358551\n",
      "0.25451958179473877\n",
      "0.25452181696891785\n",
      "0.2545149028301239\n",
      "0.2545170783996582\n",
      "0.25451019406318665\n",
      "0.2545124292373657\n",
      "0.25450557470321655\n",
      "0.25450772047042847\n",
      "0.2545008957386017\n",
      "0.254503071308136\n",
      "0.2544962465763092\n",
      "0.2544983923435211\n",
      "0.2544915974140167\n",
      "0.25449371337890625\n",
      "0.25448694825172424\n",
      "0.25448906421661377\n",
      "0.25448235869407654\n",
      "0.25448447465896606\n",
      "0.2544777989387512\n",
      "0.25447991490364075\n",
      "0.2544732391834259\n",
      "0.25447532534599304\n",
      "0.2544686198234558\n",
      "0.25447070598602295\n",
      "0.2544640898704529\n",
      "0.2544662058353424\n",
      "0.25445955991744995\n",
      "0.2544616460800171\n",
      "0.254455029964447\n",
      "0.25445711612701416\n",
      "0.2544504702091217\n",
      "0.25445258617401123\n",
      "0.25444597005844116\n",
      "0.2544480562210083\n",
      "0.254441499710083\n",
      "0.25444355607032776\n",
      "0.25443702936172485\n",
      "0.2544390857219696\n",
      "0.2544325888156891\n",
      "0.25443461537361145\n",
      "0.2544281482696533\n",
      "0.25443023443222046\n",
      "0.25442376732826233\n",
      "0.2544258236885071\n",
      "0.25441938638687134\n",
      "0.2544214129447937\n",
      "0.25441503524780273\n",
      "0.2544170320034027\n",
      "0.25441062450408936\n",
      "0.25441262125968933\n",
      "0.25440630316734314\n",
      "0.2544082701206207\n",
      "0.25440192222595215\n",
      "0.25440388917922974\n",
      "0.25439757108688354\n",
      "0.25439950823783875\n",
      "0.25439321994781494\n",
      "0.2543952167034149\n",
      "0.2543889880180359\n",
      "0.2543908953666687\n",
      "0.2543846368789673\n",
      "0.2543865740299225\n",
      "0.25438031554222107\n",
      "0.25438228249549866\n",
      "0.25437605381011963\n",
      "0.25437796115875244\n",
      "0.2543717622756958\n",
      "0.254373699426651\n",
      "0.25436753034591675\n",
      "0.25436946749687195\n",
      "0.2543633282184601\n",
      "0.2543652057647705\n",
      "0.25435906648635864\n",
      "0.2543609142303467\n",
      "0.2543548345565796\n",
      "0.25435671210289\n",
      "0.25435060262680054\n",
      "0.2543524503707886\n",
      "0.25434643030166626\n",
      "0.2543482780456543\n",
      "0.2543422281742096\n",
      "0.25434410572052\n",
      "0.2543380856513977\n",
      "0.25433990359306335\n",
      "0.2543339431285858\n",
      "0.25433576107025146\n",
      "0.2543298006057739\n",
      "0.2543315887451172\n",
      "0.25432565808296204\n",
      "0.2543274760246277\n",
      "0.2543216049671173\n",
      "0.25432342290878296\n",
      "0.2543174922466278\n",
      "0.25431928038597107\n",
      "0.2543133795261383\n",
      "0.2543151378631592\n",
      "0.2543092668056488\n",
      "0.25431105494499207\n",
      "0.2543051838874817\n",
      "0.25430697202682495\n",
      "0.25430113077163696\n",
      "0.25430288910865784\n",
      "0.25429707765579224\n",
      "0.2542988657951355\n",
      "0.2542930245399475\n",
      "0.2542947828769684\n",
      "0.25428900122642517\n",
      "0.25429075956344604\n",
      "0.25428497791290283\n",
      "0.2542867064476013\n",
      "0.2542809545993805\n",
      "0.25428271293640137\n",
      "0.25427696108818054\n",
      "0.25427865982055664\n",
      "0.2542729675769806\n",
      "0.25427472591400146\n",
      "0.254269003868103\n",
      "0.2542707324028015\n",
      "0.25426506996154785\n",
      "0.25426679849624634\n",
      "0.2542610764503479\n",
      "0.254262775182724\n",
      "0.25425711274147034\n",
      "0.2542588710784912\n",
      "0.25425320863723755\n",
      "0.25425487756729126\n",
      "0.25424930453300476\n",
      "0.25425097346305847\n",
      "0.254245400428772\n",
      "0.2542470693588257\n",
      "0.2542414963245392\n",
      "0.2542431950569153\n",
      "0.2542376220226288\n",
      "0.2542392909526825\n",
      "0.2542337477207184\n",
      "0.2542353868484497\n",
      "0.2542298436164856\n",
      "0.2542315125465393\n",
      "0.2542259991168976\n",
      "0.2542276382446289\n",
      "0.25422221422195435\n",
      "0.25422385334968567\n",
      "0.25421836972236633\n",
      "0.25422000885009766\n",
      "0.2542145252227783\n",
      "0.25421616435050964\n",
      "0.2542107403278351\n",
      "0.2542123794555664\n",
      "0.25420695543289185\n",
      "0.2542085647583008\n",
      "0.2542031407356262\n",
      "0.25420477986335754\n",
      "0.25419941544532776\n",
      "0.2542009651660919\n",
      "0.2541956603527069\n",
      "0.25419721007347107\n",
      "0.25419187545776367\n",
      "0.25419342517852783\n",
      "0.25418809056282043\n",
      "0.25418969988822937\n",
      "0.25418442487716675\n",
      "0.25418591499328613\n",
      "0.2541806697845459\n",
      "0.2541821599006653\n",
      "0.25417688488960266\n",
      "0.25417840480804443\n",
      "0.2541731595993042\n",
      "0.25417470932006836\n",
      "0.2541694939136505\n",
      "0.2541710138320923\n",
      "0.25416576862335205\n",
      "0.2541673183441162\n",
      "0.25416210293769836\n",
      "0.25416359305381775\n",
      "0.2541584074497223\n",
      "0.25415992736816406\n",
      "0.254154771566391\n",
      "0.254156231880188\n",
      "0.2541511356830597\n",
      "0.25415265560150146\n",
      "0.2541475296020508\n",
      "0.25414904952049255\n",
      "0.25414395332336426\n",
      "0.25414541363716125\n",
      "0.25414034724235535\n",
      "0.25414183735847473\n",
      "0.25413674116134644\n",
      "0.25413820147514343\n",
      "0.2541331350803375\n",
      "0.25413456559181213\n",
      "0.254129558801651\n",
      "0.2541309595108032\n",
      "0.2541259229183197\n",
      "0.2541274130344391\n",
      "0.2541223466396332\n",
      "0.2541238069534302\n",
      "0.25411877036094666\n",
      "0.25412023067474365\n",
      "0.2541152834892273\n",
      "0.2541167438030243\n",
      "0.25411173701286316\n",
      "0.25411316752433777\n",
      "0.254108190536499\n",
      "0.25410962104797363\n",
      "0.25410470366477966\n",
      "0.2541061043739319\n",
      "0.2541011869907379\n",
      "0.25410258769989014\n",
      "0.2540976405143738\n",
      "0.254099041223526\n",
      "0.2540941536426544\n",
      "0.25409552454948425\n",
      "0.25409066677093506\n",
      "0.2540920376777649\n",
      "0.2540871798992157\n",
      "0.2540885806083679\n",
      "0.2540837526321411\n",
      "0.25408515334129333\n",
      "0.25408029556274414\n",
      "0.254081666469574\n",
      "0.25407686829566956\n",
      "0.2540782392024994\n",
      "0.2540734112262726\n",
      "0.2540748417377472\n",
      "0.2540700435638428\n",
      "0.2540713846683502\n",
      "0.2540665566921234\n",
      "0.25406795740127563\n",
      "0.254063218832016\n",
      "0.25406455993652344\n",
      "0.2540597915649414\n",
      "0.254061222076416\n",
      "0.2540564239025116\n",
      "0.2540578246116638\n",
      "0.2540530860424042\n",
      "0.2540544271469116\n",
      "0.25404971837997437\n",
      "0.2540510594844818\n",
      "0.25404635071754456\n",
      "0.2540477216243744\n",
      "0.25404298305511475\n",
      "0.2540443539619446\n",
      "0.2540396749973297\n",
      "0.25404101610183716\n",
      "0.2540363669395447\n",
      "0.2540377080440521\n",
      "0.25403302907943726\n",
      "0.2540343403816223\n",
      "0.2540297210216522\n",
      "0.25403106212615967\n",
      "0.2540264427661896\n",
      "0.25402772426605225\n",
      "0.25402313470840454\n",
      "0.254024475812912\n",
      "0.2540198564529419\n",
      "0.25402116775512695\n",
      "0.25401657819747925\n",
      "0.2540178894996643\n",
      "0.254013329744339\n",
      "0.25401461124420166\n",
      "0.25401002168655396\n",
      "0.2540113031864166\n",
      "0.2540067434310913\n",
      "0.254008024930954\n",
      "0.25400349497795105\n",
      "0.2540047764778137\n",
      "0.2540002763271332\n",
      "0.25400155782699585\n",
      "0.2539970278739929\n",
      "0.253998339176178\n",
      "0.25399380922317505\n",
      "0.2539951205253601\n",
      "0.25399062037467957\n",
      "0.25399190187454224\n",
      "0.2539874017238617\n",
      "0.253988653421402\n",
      "0.2539842426776886\n",
      "0.2539854347705841\n",
      "0.25398099422454834\n",
      "0.25398221611976624\n",
      "0.25397780537605286\n",
      "0.2539790868759155\n",
      "0.2539746165275574\n",
      "0.25397586822509766\n",
      "0.2539714574813843\n",
      "0.2539726793766022\n",
      "0.25396832823753357\n",
      "0.25396955013275146\n",
      "0.2539651393890381\n",
      "0.2539663314819336\n",
      "0.2539619505405426\n",
      "0.2539631724357605\n",
      "0.2539588510990143\n",
      "0.2539600729942322\n",
      "0.2539557218551636\n",
      "0.2539568841457367\n",
      "0.2539525628089905\n",
      "0.2539537847042084\n",
      "0.25394943356513977\n",
      "0.2539506256580353\n",
      "0.25394633412361145\n",
      "0.25394755601882935\n",
      "0.25394323468208313\n",
      "0.253944456577301\n",
      "0.2539401948451996\n",
      "0.2539413571357727\n",
      "0.2539370357990265\n",
      "0.2539382576942444\n",
      "0.25393396615982056\n",
      "0.25393515825271606\n",
      "0.2539308965206146\n",
      "0.25393208861351013\n",
      "0.2539278566837311\n",
      "0.2539289891719818\n",
      "0.25392475724220276\n",
      "0.2539259195327759\n",
      "0.2539217174053192\n",
      "0.25392287969589233\n",
      "0.25391870737075806\n",
      "0.2539198398590088\n",
      "0.2539156973361969\n",
      "0.25391680002212524\n",
      "0.25391265749931335\n",
      "0.2539137601852417\n",
      "0.2539096474647522\n",
      "0.25391075015068054\n",
      "0.25390660762786865\n",
      "0.2539077699184418\n",
      "0.2539035975933075\n",
      "0.25390470027923584\n",
      "0.25390058755874634\n",
      "0.25390172004699707\n",
      "0.25389760732650757\n",
      "0.2538987398147583\n",
      "0.2538946568965912\n",
      "0.2538957893848419\n",
      "0.2538916766643524\n",
      "0.25389283895492554\n",
      "0.25388872623443604\n",
      "0.25388985872268677\n",
      "0.25388580560684204\n",
      "0.2538869082927704\n",
      "0.25388285517692566\n",
      "0.253883957862854\n",
      "0.2538799047470093\n",
      "0.2538810074329376\n",
      "0.2538769543170929\n",
      "0.25387805700302124\n",
      "0.2538740336894989\n",
      "0.25387513637542725\n",
      "0.2538711130619049\n",
      "0.25387221574783325\n",
      "0.2538682222366333\n",
      "0.25386926531791687\n",
      "0.2538652718067169\n",
      "0.2538663446903229\n",
      "0.2538623511791229\n",
      "0.2538634240627289\n",
      "0.2538594603538513\n",
      "0.25386056303977966\n",
      "0.2538565695285797\n",
      "0.25385764241218567\n",
      "0.2538536489009857\n",
      "0.25385475158691406\n",
      "0.2538508176803589\n",
      "0.25385189056396484\n",
      "0.2538478970527649\n",
      "0.25384896993637085\n",
      "0.2538450360298157\n",
      "0.253846138715744\n",
      "0.25384217500686646\n",
      "0.2538432478904724\n",
      "0.253839373588562\n",
      "0.2538403868675232\n",
      "0.2538364827632904\n",
      "0.253837525844574\n",
      "0.2538336515426636\n",
      "0.25383469462394714\n",
      "0.25383082032203674\n",
      "0.2538318634033203\n",
      "0.2538279891014099\n",
      "0.2538290321826935\n",
      "0.25382518768310547\n",
      "0.25382623076438904\n",
      "0.253822386264801\n",
      "0.2538234293460846\n",
      "0.25381964445114136\n",
      "0.25382065773010254\n",
      "0.2538168430328369\n",
      "0.2538178265094757\n",
      "0.2538140118122101\n",
      "0.25381505489349365\n",
      "0.253811240196228\n",
      "0.2538122832775116\n",
      "0.25380846858024597\n",
      "0.25380951166152954\n",
      "0.25380566716194153\n",
      "0.2538067102432251\n",
      "0.25380292534828186\n",
      "0.25380390882492065\n",
      "0.2538001537322998\n",
      "0.2538011372089386\n",
      "0.25379741191864014\n",
      "0.25379836559295654\n",
      "0.2537946403026581\n",
      "0.25379565358161926\n",
      "0.2537919282913208\n",
      "0.2537929117679596\n",
      "0.25378918647766113\n",
      "0.2537901997566223\n",
      "0.25378644466400146\n",
      "0.25378745794296265\n",
      "0.2537837624549866\n",
      "0.25378474593162537\n",
      "0.2537810802459717\n",
      "0.2537820339202881\n",
      "0.253778338432312\n",
      "0.2537792921066284\n",
      "0.25377559661865234\n",
      "0.2537766098976135\n",
      "0.25377294421195984\n",
      "0.25377392768859863\n",
      "0.25377026200294495\n",
      "0.25377127528190613\n",
      "0.25376760959625244\n",
      "0.25376856327056885\n",
      "0.2537648677825928\n",
      "0.25376585125923157\n",
      "0.25376221537590027\n",
      "0.25376319885253906\n",
      "0.2537595331668854\n",
      "0.25376051664352417\n",
      "0.25375691056251526\n",
      "0.2537578344345093\n",
      "0.25375422835350037\n",
      "0.2537551820278168\n",
      "0.25375163555145264\n",
      "0.25375255942344666\n",
      "0.25374898314476013\n",
      "0.25374990701675415\n",
      "0.2537463307380676\n",
      "0.25374725461006165\n",
      "0.2537437081336975\n",
      "0.25374460220336914\n",
      "0.2537410855293274\n",
      "0.253741979598999\n",
      "0.2537384629249573\n",
      "0.2537393867969513\n",
      "0.25373584032058716\n",
      "0.25373679399490356\n",
      "0.25373324751853943\n",
      "0.25373417139053345\n",
      "0.2537306547164917\n",
      "0.2537315785884857\n",
      "0.25372809171676636\n",
      "0.253728985786438\n",
      "0.25372546911239624\n",
      "0.25372642278671265\n",
      "0.2537229359149933\n",
      "0.2537238299846649\n",
      "0.25372034311294556\n",
      "0.2537212371826172\n",
      "0.2537177801132202\n",
      "0.25371864438056946\n",
      "0.25371524691581726\n",
      "0.2537161409854889\n",
      "0.25371262431144714\n",
      "0.25371354818344116\n",
      "0.2537100911140442\n",
      "0.2537109851837158\n",
      "0.25370755791664124\n",
      "0.25370845198631287\n",
      "0.2537050247192383\n",
      "0.2537059187889099\n",
      "0.25370246171951294\n",
      "0.25370341539382935\n",
      "0.25369998812675476\n",
      "0.253700852394104\n",
      "0.2536974549293518\n",
      "0.25369834899902344\n",
      "0.25369495153427124\n",
      "0.2536957859992981\n",
      "0.2536924183368683\n",
      "0.25369325280189514\n",
      "0.2536899149417877\n",
      "0.2536907494068146\n",
      "0.25368741154670715\n",
      "0.2536882162094116\n",
      "0.2536848783493042\n",
      "0.25368571281433105\n",
      "0.25368234515190125\n",
      "0.2536832094192505\n",
      "0.25367987155914307\n",
      "0.2536807358264923\n",
      "0.2536774277687073\n",
      "0.25367826223373413\n",
      "0.2536748945713043\n",
      "0.25367581844329834\n",
      "0.25367245078086853\n",
      "0.25367334485054016\n",
      "0.25367000699043274\n",
      "0.2536708116531372\n",
      "0.25366753339767456\n",
      "0.2536683678627014\n",
      "0.25366508960723877\n",
      "0.2536659240722656\n",
      "0.253662645816803\n",
      "0.25366348028182983\n",
      "0.2536601722240448\n",
      "0.25366103649139404\n",
      "0.253657728433609\n",
      "0.2536585330963135\n",
      "0.2536552846431732\n",
      "0.2536561191082001\n",
      "0.2536529004573822\n",
      "0.25365370512008667\n",
      "0.2536504864692688\n",
      "0.25365132093429565\n",
      "0.2536480724811554\n",
      "0.2536488175392151\n",
      "0.2536455988883972\n",
      "0.25364646315574646\n",
      "0.2536432147026062\n",
      "0.25364407896995544\n",
      "0.2536408603191376\n",
      "0.25364163517951965\n",
      "0.2536384165287018\n",
      "0.25363925099372864\n",
      "0.25363603234291077\n",
      "0.25363683700561523\n",
      "0.25363364815711975\n",
      "0.25363442301750183\n",
      "0.25363126397132874\n",
      "0.2536320686340332\n",
      "0.2536288797855377\n",
      "0.2536296844482422\n",
      "0.2536264955997467\n",
      "0.25362730026245117\n",
      "0.2536241114139557\n",
      "0.25362488627433777\n",
      "0.25362175703048706\n",
      "0.25362253189086914\n",
      "0.25361940264701843\n",
      "0.2536202073097229\n",
      "0.25361698865890503\n",
      "0.2536177933216095\n",
      "0.2536146640777588\n",
      "0.25361546874046326\n",
      "0.25361233949661255\n",
      "0.25361308455467224\n",
      "0.25360995531082153\n",
      "0.253610759973526\n",
      "0.25360769033432007\n",
      "0.2536084055900574\n",
      "0.25360536575317383\n",
      "0.25360608100891113\n",
      "0.2536030411720276\n",
      "0.2536037862300873\n",
      "0.25360071659088135\n",
      "0.2536014914512634\n",
      "0.2535983622074127\n",
      "0.2535991668701172\n",
      "0.25359606742858887\n",
      "0.25359678268432617\n",
      "0.2535938024520874\n",
      "0.2535945177078247\n",
      "0.2535914480686188\n",
      "0.25359222292900085\n",
      "0.2535891532897949\n",
      "0.253589928150177\n",
      "0.25358685851097107\n",
      "0.25358766317367554\n",
      "0.2535845935344696\n",
      "0.2535853385925293\n",
      "0.2535823583602905\n",
      "0.25358307361602783\n",
      "0.25358009338378906\n",
      "0.253580778837204\n",
      "0.2535777986049652\n",
      "0.2535785436630249\n",
      "0.25357556343078613\n",
      "0.25357627868652344\n",
      "0.25357329845428467\n",
      "0.253574013710022\n",
      "0.2535710632801056\n",
      "0.2535717487335205\n",
      "0.2535687983036041\n",
      "0.2535695433616638\n",
      "0.25356656312942505\n",
      "0.25356727838516235\n",
      "0.25356432795524597\n",
      "0.2535650134086609\n",
      "0.2535620629787445\n",
      "0.2535628080368042\n",
      "0.2535598874092102\n",
      "0.2535606026649475\n",
      "0.25355765223503113\n",
      "0.25355833768844604\n",
      "0.25355544686317444\n",
      "0.25355613231658936\n",
      "0.253553181886673\n",
      "0.25355392694473267\n",
      "0.25355103611946106\n",
      "0.253551721572876\n",
      "0.253548800945282\n",
      "0.2535495162010193\n",
      "0.2535466253757477\n",
      "0.2535473108291626\n",
      "0.2535444498062134\n",
      "0.2535451054573059\n",
      "0.2535422444343567\n",
      "0.2535429000854492\n",
      "0.2535400390625\n",
      "0.2535407841205597\n",
      "0.2535378932952881\n",
      "0.253538578748703\n",
      "0.253535658121109\n",
      "0.2535364031791687\n",
      "0.2535335421562195\n",
      "0.253534197807312\n",
      "0.25353139638900757\n",
      "0.2535320520401001\n",
      "0.2535291910171509\n",
      "0.2535299062728882\n",
      "0.25352704524993896\n",
      "0.2535277009010315\n",
      "0.25352492928504944\n",
      "0.25352561473846436\n",
      "0.25352275371551514\n",
      "0.25352343916893005\n",
      "0.2535206377506256\n",
      "0.25352129340171814\n",
      "0.2535184919834137\n",
      "0.2535191774368286\n",
      "0.2535163462162018\n",
      "0.2535170018672943\n",
      "0.25351426005363464\n",
      "0.2535148859024048\n",
      "0.25351211428642273\n",
      "0.25351274013519287\n",
      "0.2535099685192108\n",
      "0.25351062417030334\n",
      "0.2535078525543213\n",
      "0.25350847840309143\n",
      "0.2535057067871094\n",
      "0.2535063624382019\n",
      "0.25350359082221985\n",
      "0.25350427627563477\n",
      "0.2535015344619751\n",
      "0.25350216031074524\n"
     ]
    }
   ],
   "source": [
    "# Overfitting a single batch of the data\n",
    "for _ in range(1000):\n",
    "    # Forwards pass\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2\n",
    "    #counts = logits.exp()\n",
    "    #probs = counts / counts.sum(1, keepdim=True)\n",
    "    #loss = -prob[torch.arange(32),Y].log().mean()\n",
    "    # -----------> Same as\n",
    "    loss = F.cross_entropy(logits,Y)\n",
    "    print(loss.item())\n",
    "    # Backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in params:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([13.9250, 18.9960, 21.1395, 21.4166, 17.5814, 13.9250, 16.8153, 14.8652,\n",
       "        16.6063, 19.3501, 16.8656, 21.8019, 13.9250, 18.1480, 18.0803, 21.0486,\n",
       "        13.9250, 17.3920, 16.2242, 18.0663, 19.3317, 16.9361, 11.7411, 11.4472,\n",
       "        16.1109, 13.9250, 16.9722, 17.7624, 13.3431, 16.8724, 20.0886, 17.2104],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([ 9, 13, 13,  1,  0,  9, 12,  9, 22,  9,  1,  0,  9, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0,  9, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are unable to completely overfit and make the loss be completely 0 because\n",
    "of the structure where we have the start of a name \n",
    "\n",
    "... ---> [a,b,c,d,e,f,...,y,z]\n",
    "\n",
    "This is because these are supposed to be equally likely outcomes for the exact same input\n",
    "But we are very close in the cases where there is a unique input for a unique output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.0466e-04, 3.3281e-04, 6.6846e-03, 9.9208e-01])\n",
      "tensor([0., 0., 0., nan])\n"
     ]
    }
   ],
   "source": [
    "# Its better to use F cross entrop\n",
    "# Pytorh will not create intermediate tensors which is fairly inefficient\n",
    "# Instead it will cluster up all the operations and will have fused kernels\n",
    "# that very effciently evaluate these expressions that are clustered mathematical operations\n",
    "\n",
    "# Backward pass can be made more efficient\n",
    "# We don't have to apply chain rule individually through each tensor\n",
    "# instead we can go directly because the expression gets simplified mathematically\n",
    "\n",
    "# Under the hood it can be better behaved\n",
    "\n",
    "logits = torch.tensor([-2,-3,0,5])\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "print(probs)\n",
    "\n",
    "# At very positive logits you start to run into trouble\n",
    "\n",
    "logits = torch.tensor([-100,-3,0,100])\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "print(probs)\n",
    "\n",
    "# This is because the way that exp works\n",
    "# If you pass a very negative number you get a number  very near 0\n",
    "# but if you pass a very big number we run  out of range in the float that represents our values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([-100,-3,0,100]) - 100 # offsets will create the same probabilities\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "print(probs)\n",
    "# Pytorch will subtract 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the full dataset to optimize the neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavenet310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
